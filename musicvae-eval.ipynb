{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cebfefb8-3a1e-4fae-a8e0-b223be48c163",
   "metadata": {},
   "source": [
    "##### Candidate Name: Putra Bahy Helmi Hartoyo\n",
    "##### Candidate Email: bahyhelmi97@gmail.com\n",
    "\n",
    "- This notebook is written to fulfill the assesment task of PozaLabs in creating music samples from MusicVAE architecture using MIDI dataset.\n",
    "- This notebook adopt the method available on the official GitHub repository of MusicVAE: https://github.com/magenta/magenta/tree/main/magenta/models/music_vae#training-your-own-musicvae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a2378-3c2e-438a-ac33-180b3897edbe",
   "metadata": {},
   "source": [
    "#### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "215bdef3-3aea-4b8b-842e-615a097b5200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63478413-17df-4a09-b14d-6d7b542208fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542e6407-931c-4fe6-9e48-74c2c7ed3138",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39588775-e7b4-4600-8e99-4d610c3ad142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bdabe11a47144d7a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bdabe11a47144d7a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup Tensorboard to monitor validation metrics\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=ckpts/groove-4-bar/eval --port=6007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "532861a2-003f-433d-999c-8829fdb29384",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/icl1/anaconda3/envs/test/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "2023-06-02 11:43:28.831960: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-02 11:43:29.447662: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/icl1/anaconda3/envs/test/lib/:/home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/nvidia/cudnn/lib\n",
      "2023-06-02 11:43:29.447888: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/icl1/anaconda3/envs/test/lib/:/home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/nvidia/cudnn/lib\n",
      "2023-06-02 11:43:29.447896: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "WARNING:tensorflow:From /home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2023-06-02 11:43:37.080333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:43:37.104360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:43:37.104547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:43:37.105770: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n",
      "I0602 11:43:44.363777 140292327446336 dataset_info.py:491] Load dataset info from gs://tfds-data/datasets/groove/4bar-midionly/2.0.1\n",
      "I0602 11:43:50.814993 140292327446336 dataset_info.py:552] Field info.description from disk and from code do not match. Keeping the one from code.\n",
      "I0602 11:43:50.815110 140292327446336 dataset_info.py:552] Field info.config_name from disk and from code do not match. Keeping the one from code.\n",
      "I0602 11:43:50.815152 140292327446336 dataset_info.py:552] Field info.config_description from disk and from code do not match. Keeping the one from code.\n",
      "I0602 11:43:50.815191 140292327446336 dataset_info.py:552] Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "I0602 11:43:50.815268 140292327446336 dataset_info.py:552] Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "I0602 11:43:50.815307 140292327446336 dataset_info.py:552] Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "I0602 11:43:52.786599 140292327446336 dataset_builder.py:383] Reusing dataset groove (gs://tfds-data/datasets/groove/4bar-midionly/2.0.1)\n",
      "I0602 11:43:52.786744 140292327446336 logging_logger.py:45] Constructing tf.data.Dataset groove for split validation, from gs://tfds-data/datasets/groove/4bar-midionly/2.0.1\n",
      "2023-06-02 11:43:53.667304: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-02 11:43:53.668903: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "INFO:tensorflow:Total examples: 1929\n",
      "I0602 11:44:02.459396 140292327446336 data.py:1705] Total examples: 1929\n",
      "2023-06-02 11:44:02.462512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:44:02.462692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:44:02.462795: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:44:02.862927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:44:02.863097: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:44:02.863209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:44:02.863300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, GrooveLstmDecoder, and hparams:\n",
      "{'max_seq_len': 64, 'z_size': 256, 'free_bits': 48, 'max_beta': 0.2, 'beta_rate': 0.0, 'batch_size': 512, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [256, 256], 'enc_rnn_size': [512], 'dropout_keep_prob': 0.3, 'sampling_schedule': 'constant', 'sampling_rate': 0.0, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False, 'control_preprocessing_rnn_size': [256]}\n",
      "I0602 11:44:02.880560 140292327446336 base_model.py:152] Building MusicVAE model with BidirectionalLstmEncoder, GrooveLstmDecoder, and hparams:\n",
      "{'max_seq_len': 64, 'z_size': 256, 'free_bits': 48, 'max_beta': 0.2, 'beta_rate': 0.0, 'batch_size': 512, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [256, 256], 'enc_rnn_size': [512], 'dropout_keep_prob': 0.3, 'sampling_schedule': 'constant', 'sampling_rate': 0.0, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False, 'control_preprocessing_rnn_size': [256]}\n",
      "INFO:tensorflow:\n",
      "Encoder Cells (bidirectional):\n",
      "  units: [512]\n",
      "\n",
      "I0602 11:44:02.883080 140292327446336 lstm_models.py:78] \n",
      "Encoder Cells (bidirectional):\n",
      "  units: [512]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "W0602 11:44:02.888219 140292327446336 legacy_cells.py:1221] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "W0602 11:44:02.892725 140292327446336 legacy_cells.py:1221] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "INFO:tensorflow:\n",
      "Decoder Cells:\n",
      "  units: [256, 256]\n",
      "\n",
      "I0602 11:44:02.892919 140292327446336 lstm_models.py:224] \n",
      "Decoder Cells:\n",
      "  units: [256, 256]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "W0602 11:44:02.902079 140292327446336 legacy_cells.py:1221] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "INFO:tensorflow:Reading examples from TFDS: groove/4bar-midionly\n",
      "I0602 11:44:02.902317 140292327446336 data.py:1832] Reading examples from TFDS: groove/4bar-midionly\n",
      "I0602 11:44:09.136805 140292327446336 dataset_info.py:491] Load dataset info from gs://tfds-data/datasets/groove/4bar-midionly/2.0.1\n",
      "I0602 11:44:15.414754 140292327446336 dataset_info.py:552] Field info.description from disk and from code do not match. Keeping the one from code.\n",
      "I0602 11:44:15.414876 140292327446336 dataset_info.py:552] Field info.config_name from disk and from code do not match. Keeping the one from code.\n",
      "I0602 11:44:15.414924 140292327446336 dataset_info.py:552] Field info.config_description from disk and from code do not match. Keeping the one from code.\n",
      "I0602 11:44:15.414961 140292327446336 dataset_info.py:552] Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "I0602 11:44:15.415035 140292327446336 dataset_info.py:552] Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "I0602 11:44:15.415075 140292327446336 dataset_info.py:552] Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "I0602 11:44:16.788675 140292327446336 dataset_builder.py:383] Reusing dataset groove (gs://tfds-data/datasets/groove/4bar-midionly/2.0.1)\n",
      "I0602 11:44:16.788804 140292327446336 logging_logger.py:45] Constructing tf.data.Dataset groove for split validation, from gs://tfds-data/datasets/groove/4bar-midionly/2.0.1\n",
      "WARNING:tensorflow:From /home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "W0602 11:44:17.565139 140292327446336 deprecation.py:356] From /home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:From /home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0602 11:44:17.642506 140292327446336 deprecation.py:356] From /home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/magenta/contrib/rnn.py:473: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "W0602 11:44:17.653660 140292327446336 deprecation.py:356] From /home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/magenta/contrib/rnn.py:473: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:446: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0602 11:44:17.653859 140292327446336 deprecation.py:356] From /home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:446: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "/home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/magenta/contrib/rnn.py:750: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self._names[\"W\"], [input_size + self._num_units, self._num_units * 4])\n",
      "/home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/magenta/contrib/rnn.py:753: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  initializer=tf.constant_initializer(0.0))\n",
      "/home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/magenta/models/music_vae/base_model.py:199: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n",
      "/home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/magenta/models/music_vae/base_model.py:205: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n",
      "WARNING:tensorflow:From /home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow_probability/python/distributions/distribution.py:342: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n",
      "W0602 11:44:17.892250 140292327446336 deprecation.py:560] From /home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow_probability/python/distributions/distribution.py:342: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n",
      "/home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/magenta/models/music_vae/lstm_utils.py:99: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  name=name),\n",
      "INFO:tensorflow:Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "I0602 11:44:20.456480 140292327446336 evaluation.py:190] Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.checkpoint_management has been moved to tensorflow.python.checkpoint.checkpoint_management. The old module will be deleted in version 2.9.\n",
      "W0602 11:44:20.456590 140292327446336 deprecation.py:702] Please fix your imports. Module tensorflow.python.training.checkpoint_management has been moved to tensorflow.python.checkpoint.checkpoint_management. The old module will be deleted in version 2.9.\n",
      "INFO:tensorflow:Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-0\n",
      "I0602 11:44:23.461016 140292327446336 evaluation.py:199] Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-0\n",
      "2023-06-02 11:44:23.505985: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:44:23.506230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:44:23.506340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:44:23.506490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:44:23.506601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:44:23.506677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0602 11:44:23.518115 140292327446336 monitored_session.py:240] Graph was finalized.\n",
      "2023-06-02 11:44:23.518615: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:44:23.518829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:44:23.518939: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:44:23.519081: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:44:23.519188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:44:23.519263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-0\n",
      "I0602 11:44:23.519435 140292327446336 saver.py:1410] Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-0\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0602 11:44:23.969823 140292327446336 session_manager.py:526] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0602 11:44:24.028649 140292327446336 session_manager.py:529] Done running local_init_op.\n",
      "INFO:tensorflow:Starting evaluation at 2023-06-02-02:44:24\n",
      "I0602 11:44:24.681088 140292327446336 evaluation.py:451] Starting evaluation at 2023-06-02-02:44:24\n",
      "2023-06-02 11:44:31.074983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "I0602 11:44:31.124706 140292327446336 evaluation.py:163] Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "I0602 11:44:33.295993 140292327446336 evaluation.py:163] Evaluation [2/3]\n",
      "2023-06-02 11:44:35.798674: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-06-02 11:44:35.819096: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "I0602 11:44:35.825704 140292327446336 evaluation.py:163] Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2023-06-02-02:44:35\n",
      "I0602 11:44:35.825876 140292327446336 evaluation.py:457] Finished evaluation at 2023-06-02-02:44:35\n",
      "INFO:tensorflow:Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "I0602 11:45:23.505983 140292327446336 evaluation.py:190] Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "INFO:tensorflow:Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-12\n",
      "I0602 11:45:26.510563 140292327446336 evaluation.py:199] Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-12\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0602 11:45:26.511405 140292327446336 monitored_session.py:240] Graph was finalized.\n",
      "2023-06-02 11:45:26.511835: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:45:26.512053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:45:26.512154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:45:26.512297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:45:26.512400: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:45:26.512479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-12\n",
      "I0602 11:45:26.512619 140292327446336 saver.py:1410] Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-12\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0602 11:45:26.803654 140292327446336 session_manager.py:526] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0602 11:45:26.863313 140292327446336 session_manager.py:529] Done running local_init_op.\n",
      "INFO:tensorflow:Starting evaluation at 2023-06-02-02:45:26\n",
      "I0602 11:45:26.928173 140292327446336 evaluation.py:451] Starting evaluation at 2023-06-02-02:45:26\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "I0602 11:45:33.608626 140292327446336 evaluation.py:163] Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "I0602 11:45:35.918668 140292327446336 evaluation.py:163] Evaluation [2/3]\n",
      "2023-06-02 11:45:38.429472: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-06-02 11:45:38.457600: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "I0602 11:45:38.458787 140292327446336 evaluation.py:163] Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2023-06-02-02:45:38\n",
      "I0602 11:45:38.458939 140292327446336 evaluation.py:457] Finished evaluation at 2023-06-02-02:45:38\n",
      "INFO:tensorflow:Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "I0602 11:46:26.558038 140292327446336 evaluation.py:190] Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "INFO:tensorflow:Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-470\n",
      "I0602 11:46:26.558872 140292327446336 evaluation.py:199] Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-470\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0602 11:46:26.559637 140292327446336 monitored_session.py:240] Graph was finalized.\n",
      "2023-06-02 11:46:26.560035: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:46:26.560240: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:46:26.560348: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:46:26.560490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:46:26.560598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:46:26.560677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-470\n",
      "I0602 11:46:26.560788 140292327446336 saver.py:1410] Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-470\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0602 11:46:26.815952 140292327446336 session_manager.py:526] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0602 11:46:26.877424 140292327446336 session_manager.py:529] Done running local_init_op.\n",
      "INFO:tensorflow:Starting evaluation at 2023-06-02-02:46:26\n",
      "I0602 11:46:26.941608 140292327446336 evaluation.py:451] Starting evaluation at 2023-06-02-02:46:26\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "I0602 11:46:33.341806 140292327446336 evaluation.py:163] Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "I0602 11:46:35.853055 140292327446336 evaluation.py:163] Evaluation [2/3]\n",
      "2023-06-02 11:46:38.327503: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "I0602 11:46:38.350740 140292327446336 evaluation.py:163] Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2023-06-02-02:46:38\n",
      "I0602 11:46:38.353757 140292327446336 evaluation.py:457] Finished evaluation at 2023-06-02-02:46:38\n",
      "2023-06-02 11:46:38.359908: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "I0602 11:47:26.607060 140292327446336 evaluation.py:190] Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "INFO:tensorflow:Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-1164\n",
      "I0602 11:47:26.608009 140292327446336 evaluation.py:199] Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-1164\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0602 11:47:26.608752 140292327446336 monitored_session.py:240] Graph was finalized.\n",
      "2023-06-02 11:47:26.609155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:47:26.609352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:47:26.609459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:47:26.609600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:47:26.609719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:47:26.609797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-1164\n",
      "I0602 11:47:26.609910 140292327446336 saver.py:1410] Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-1164\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0602 11:47:26.867676 140292327446336 session_manager.py:526] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0602 11:47:26.927479 140292327446336 session_manager.py:529] Done running local_init_op.\n",
      "INFO:tensorflow:Starting evaluation at 2023-06-02-02:47:26\n",
      "I0602 11:47:26.995661 140292327446336 evaluation.py:451] Starting evaluation at 2023-06-02-02:47:26\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "I0602 11:47:33.261050 140292327446336 evaluation.py:163] Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "I0602 11:47:35.838615 140292327446336 evaluation.py:163] Evaluation [2/3]\n",
      "2023-06-02 11:47:38.334929: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-06-02 11:47:38.359590: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "I0602 11:47:38.370473 140292327446336 evaluation.py:163] Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2023-06-02-02:47:38\n",
      "I0602 11:47:38.370632 140292327446336 evaluation.py:457] Finished evaluation at 2023-06-02-02:47:38\n",
      "INFO:tensorflow:Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "I0602 11:48:26.653988 140292327446336 evaluation.py:190] Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "INFO:tensorflow:Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-1859\n",
      "I0602 11:48:26.654751 140292327446336 evaluation.py:199] Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-1859\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0602 11:48:26.655474 140292327446336 monitored_session.py:240] Graph was finalized.\n",
      "2023-06-02 11:48:26.655853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:48:26.656045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:48:26.656148: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:48:26.656283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:48:26.656387: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:48:26.656461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-1859\n",
      "I0602 11:48:26.656582 140292327446336 saver.py:1410] Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-1859\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0602 11:48:26.904014 140292327446336 session_manager.py:526] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0602 11:48:26.963153 140292327446336 session_manager.py:529] Done running local_init_op.\n",
      "INFO:tensorflow:Starting evaluation at 2023-06-02-02:48:27\n",
      "I0602 11:48:27.027595 140292327446336 evaluation.py:451] Starting evaluation at 2023-06-02-02:48:27\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "I0602 11:48:32.961249 140292327446336 evaluation.py:163] Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "I0602 11:48:35.486277 140292327446336 evaluation.py:163] Evaluation [2/3]\n",
      "2023-06-02 11:48:37.899468: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-06-02 11:48:37.921739: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "I0602 11:48:37.936282 140292327446336 evaluation.py:163] Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2023-06-02-02:48:37\n",
      "I0602 11:48:37.936446 140292327446336 evaluation.py:457] Finished evaluation at 2023-06-02-02:48:37\n",
      "INFO:tensorflow:Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "I0602 11:49:26.701985 140292327446336 evaluation.py:190] Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "INFO:tensorflow:Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-2555\n",
      "I0602 11:49:27.030715 140292327446336 evaluation.py:199] Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-2555\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0602 11:49:27.031508 140292327446336 monitored_session.py:240] Graph was finalized.\n",
      "2023-06-02 11:49:27.031906: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:49:27.032094: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:49:27.032198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:49:27.032336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:49:27.032439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:49:27.032515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-2555\n",
      "I0602 11:49:27.032641 140292327446336 saver.py:1410] Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-2555\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0602 11:49:27.270563 140292327446336 session_manager.py:526] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0602 11:49:27.330885 140292327446336 session_manager.py:529] Done running local_init_op.\n",
      "INFO:tensorflow:Starting evaluation at 2023-06-02-02:49:27\n",
      "I0602 11:49:27.400128 140292327446336 evaluation.py:451] Starting evaluation at 2023-06-02-02:49:27\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "I0602 11:49:33.320201 140292327446336 evaluation.py:163] Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "I0602 11:49:35.834799 140292327446336 evaluation.py:163] Evaluation [2/3]\n",
      "2023-06-02 11:49:38.305867: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-06-02 11:49:38.329169: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "I0602 11:49:38.341868 140292327446336 evaluation.py:163] Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2023-06-02-02:49:38\n",
      "I0602 11:49:38.342037 140292327446336 evaluation.py:457] Finished evaluation at 2023-06-02-02:49:38\n",
      "INFO:tensorflow:Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "I0602 11:50:27.078093 140292327446336 evaluation.py:190] Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "INFO:tensorflow:Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-3244\n",
      "I0602 11:50:27.078917 140292327446336 evaluation.py:199] Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-3244\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0602 11:50:27.079634 140292327446336 monitored_session.py:240] Graph was finalized.\n",
      "2023-06-02 11:50:27.080049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:50:27.080336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:50:27.080489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:50:27.080678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:50:27.080837: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:50:27.080930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-3244\n",
      "I0602 11:50:27.081064 140292327446336 saver.py:1410] Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-3244\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0602 11:50:27.330782 140292327446336 session_manager.py:526] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0602 11:50:27.391642 140292327446336 session_manager.py:529] Done running local_init_op.\n",
      "INFO:tensorflow:Starting evaluation at 2023-06-02-02:50:27\n",
      "I0602 11:50:27.459642 140292327446336 evaluation.py:451] Starting evaluation at 2023-06-02-02:50:27\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "I0602 11:50:33.345308 140292327446336 evaluation.py:163] Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "I0602 11:50:35.678928 140292327446336 evaluation.py:163] Evaluation [2/3]\n",
      "2023-06-02 11:50:38.208376: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-06-02 11:50:38.234111: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "I0602 11:50:38.236973 140292327446336 evaluation.py:163] Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2023-06-02-02:50:38\n",
      "I0602 11:50:38.237177 140292327446336 evaluation.py:457] Finished evaluation at 2023-06-02-02:50:38\n",
      "INFO:tensorflow:Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "I0602 11:51:27.125993 140292327446336 evaluation.py:190] Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "INFO:tensorflow:Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-3940\n",
      "I0602 11:51:27.127058 140292327446336 evaluation.py:199] Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-3940\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0602 11:51:27.127886 140292327446336 monitored_session.py:240] Graph was finalized.\n",
      "2023-06-02 11:51:27.128288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:51:27.128518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:51:27.128667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:51:27.128807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:51:27.128918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:51:27.128994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-3940\n",
      "I0602 11:51:27.129128 140292327446336 saver.py:1410] Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-3940\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0602 11:51:27.380141 140292327446336 session_manager.py:526] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0602 11:51:27.442185 140292327446336 session_manager.py:529] Done running local_init_op.\n",
      "INFO:tensorflow:Starting evaluation at 2023-06-02-02:51:27\n",
      "I0602 11:51:27.514181 140292327446336 evaluation.py:451] Starting evaluation at 2023-06-02-02:51:27\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "I0602 11:51:33.592773 140292327446336 evaluation.py:163] Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "I0602 11:51:36.112490 140292327446336 evaluation.py:163] Evaluation [2/3]\n",
      "2023-06-02 11:51:38.468669: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-06-02 11:51:38.495453: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "I0602 11:51:38.496724 140292327446336 evaluation.py:163] Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2023-06-02-02:51:38\n",
      "I0602 11:51:38.496876 140292327446336 evaluation.py:457] Finished evaluation at 2023-06-02-02:51:38\n",
      "INFO:tensorflow:Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "I0602 11:52:27.173990 140292327446336 evaluation.py:190] Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "INFO:tensorflow:Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-4634\n",
      "I0602 11:52:27.174934 140292327446336 evaluation.py:199] Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-4634\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0602 11:52:27.175786 140292327446336 monitored_session.py:240] Graph was finalized.\n",
      "2023-06-02 11:52:27.176213: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:52:27.177021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:52:27.177141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:52:27.177295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:52:27.177415: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:52:27.177496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-4634\n",
      "I0602 11:52:27.177644 140292327446336 saver.py:1410] Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-4634\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0602 11:52:27.432159 140292327446336 session_manager.py:526] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0602 11:52:27.495553 140292327446336 session_manager.py:529] Done running local_init_op.\n",
      "INFO:tensorflow:Starting evaluation at 2023-06-02-02:52:27\n",
      "I0602 11:52:27.561774 140292327446336 evaluation.py:451] Starting evaluation at 2023-06-02-02:52:27\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "I0602 11:52:33.753109 140292327446336 evaluation.py:163] Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "I0602 11:52:36.239304 140292327446336 evaluation.py:163] Evaluation [2/3]\n",
      "2023-06-02 11:52:38.700221: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-06-02 11:52:38.732526: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "I0602 11:52:38.737477 140292327446336 evaluation.py:163] Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2023-06-02-02:52:38\n",
      "I0602 11:52:38.737705 140292327446336 evaluation.py:457] Finished evaluation at 2023-06-02-02:52:38\n",
      "INFO:tensorflow:Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "I0602 11:53:27.221988 140292327446336 evaluation.py:190] Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "INFO:tensorflow:Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-5329\n",
      "I0602 11:53:27.366140 140292327446336 evaluation.py:199] Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-5329\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0602 11:53:27.366932 140292327446336 monitored_session.py:240] Graph was finalized.\n",
      "2023-06-02 11:53:27.367338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:53:27.367549: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:53:27.367657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:53:27.367805: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:53:27.367917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:53:27.368001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-5329\n",
      "I0602 11:53:27.368119 140292327446336 saver.py:1410] Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-5329\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0602 11:53:27.605811 140292327446336 session_manager.py:526] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0602 11:53:27.669995 140292327446336 session_manager.py:529] Done running local_init_op.\n",
      "INFO:tensorflow:Starting evaluation at 2023-06-02-02:53:27\n",
      "I0602 11:53:27.735621 140292327446336 evaluation.py:451] Starting evaluation at 2023-06-02-02:53:27\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "I0602 11:53:33.535054 140292327446336 evaluation.py:163] Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "I0602 11:53:35.964115 140292327446336 evaluation.py:163] Evaluation [2/3]\n",
      "2023-06-02 11:53:38.487142: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-06-02 11:53:38.511679: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "I0602 11:53:38.514772 140292327446336 evaluation.py:163] Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2023-06-02-02:53:38\n",
      "I0602 11:53:38.514939 140292327446336 evaluation.py:457] Finished evaluation at 2023-06-02-02:53:38\n",
      "INFO:tensorflow:Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "I0602 11:54:27.414998 140292327446336 evaluation.py:190] Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "INFO:tensorflow:Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-6016\n",
      "I0602 11:54:27.415983 140292327446336 evaluation.py:199] Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-6016\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0602 11:54:27.416747 140292327446336 monitored_session.py:240] Graph was finalized.\n",
      "2023-06-02 11:54:27.417167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:54:27.417435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:54:27.417547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:54:27.417696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:54:27.417807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:54:27.417884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-6016\n",
      "I0602 11:54:27.418016 140292327446336 saver.py:1410] Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-6016\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0602 11:54:27.674360 140292327446336 session_manager.py:526] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0602 11:54:27.738216 140292327446336 session_manager.py:529] Done running local_init_op.\n",
      "INFO:tensorflow:Starting evaluation at 2023-06-02-02:54:27\n",
      "I0602 11:54:27.813013 140292327446336 evaluation.py:451] Starting evaluation at 2023-06-02-02:54:27\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "I0602 11:54:33.941455 140292327446336 evaluation.py:163] Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "I0602 11:54:36.301414 140292327446336 evaluation.py:163] Evaluation [2/3]\n",
      "2023-06-02 11:54:38.808959: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-06-02 11:54:38.810552: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "I0602 11:54:38.817471 140292327446336 evaluation.py:163] Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2023-06-02-02:54:38\n",
      "I0602 11:54:38.817664 140292327446336 evaluation.py:457] Finished evaluation at 2023-06-02-02:54:38\n",
      "INFO:tensorflow:Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "I0602 11:55:27.461986 140292327446336 evaluation.py:190] Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "INFO:tensorflow:Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-6704\n",
      "I0602 11:55:27.462930 140292327446336 evaluation.py:199] Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-6704\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0602 11:55:27.463652 140292327446336 monitored_session.py:240] Graph was finalized.\n",
      "2023-06-02 11:55:27.464026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:55:27.464213: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:55:27.464317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:55:27.464453: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:55:27.464557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:55:27.464632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-6704\n",
      "I0602 11:55:27.464748 140292327446336 saver.py:1410] Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-6704\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0602 11:55:27.712430 140292327446336 session_manager.py:526] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0602 11:55:27.771409 140292327446336 session_manager.py:529] Done running local_init_op.\n",
      "INFO:tensorflow:Starting evaluation at 2023-06-02-02:55:27\n",
      "I0602 11:55:27.840805 140292327446336 evaluation.py:451] Starting evaluation at 2023-06-02-02:55:27\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "I0602 11:55:33.857220 140292327446336 evaluation.py:163] Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "I0602 11:55:36.358894 140292327446336 evaluation.py:163] Evaluation [2/3]\n",
      "2023-06-02 11:55:38.810109: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-06-02 11:55:38.826787: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "I0602 11:55:38.841069 140292327446336 evaluation.py:163] Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2023-06-02-02:55:38\n",
      "I0602 11:55:38.841232 140292327446336 evaluation.py:457] Finished evaluation at 2023-06-02-02:55:38\n",
      "INFO:tensorflow:Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "I0602 11:56:27.509999 140292327446336 evaluation.py:190] Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "INFO:tensorflow:Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-7398\n",
      "I0602 11:56:27.511005 140292327446336 evaluation.py:199] Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-7398\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0602 11:56:27.511739 140292327446336 monitored_session.py:240] Graph was finalized.\n",
      "2023-06-02 11:56:27.512125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:56:27.512332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:56:27.512447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:56:27.512588: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:56:27.512766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:56:27.512843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-7398\n",
      "I0602 11:56:27.512958 140292327446336 saver.py:1410] Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-7398\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0602 11:56:27.788014 140292327446336 session_manager.py:526] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0602 11:56:27.846557 140292327446336 session_manager.py:529] Done running local_init_op.\n",
      "INFO:tensorflow:Starting evaluation at 2023-06-02-02:56:27\n",
      "I0602 11:56:27.915477 140292327446336 evaluation.py:451] Starting evaluation at 2023-06-02-02:56:27\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "I0602 11:56:33.716607 140292327446336 evaluation.py:163] Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "I0602 11:56:36.153110 140292327446336 evaluation.py:163] Evaluation [2/3]\n",
      "2023-06-02 11:56:38.685814: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "I0602 11:56:38.710518 140292327446336 evaluation.py:163] Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2023-06-02-02:56:38\n",
      "I0602 11:56:38.715223 140292327446336 evaluation.py:457] Finished evaluation at 2023-06-02-02:56:38\n",
      "2023-06-02 11:56:38.717185: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "I0602 11:57:27.558076 140292327446336 evaluation.py:190] Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "INFO:tensorflow:Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-8092\n",
      "I0602 11:57:27.943976 140292327446336 evaluation.py:199] Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-8092\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0602 11:57:27.944826 140292327446336 monitored_session.py:240] Graph was finalized.\n",
      "2023-06-02 11:57:27.945223: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:57:27.945394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:57:27.945498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:57:27.945632: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:57:27.945737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:57:27.945813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-8092\n",
      "I0602 11:57:27.949472 140292327446336 saver.py:1410] Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-8092\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0602 11:57:28.212675 140292327446336 session_manager.py:526] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0602 11:57:28.277431 140292327446336 session_manager.py:529] Done running local_init_op.\n",
      "INFO:tensorflow:Starting evaluation at 2023-06-02-02:57:28\n",
      "I0602 11:57:28.341224 140292327446336 evaluation.py:451] Starting evaluation at 2023-06-02-02:57:28\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "I0602 11:57:34.381888 140292327446336 evaluation.py:163] Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "I0602 11:57:36.917153 140292327446336 evaluation.py:163] Evaluation [2/3]\n",
      "2023-06-02 11:57:39.457801: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-06-02 11:57:39.483222: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "I0602 11:57:39.492532 140292327446336 evaluation.py:163] Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2023-06-02-02:57:39\n",
      "I0602 11:57:39.492692 140292327446336 evaluation.py:457] Finished evaluation at 2023-06-02-02:57:39\n",
      "INFO:tensorflow:Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "I0602 11:58:27.989988 140292327446336 evaluation.py:190] Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "INFO:tensorflow:Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-8786\n",
      "I0602 11:58:27.990959 140292327446336 evaluation.py:199] Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-8786\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0602 11:58:27.991663 140292327446336 monitored_session.py:240] Graph was finalized.\n",
      "2023-06-02 11:58:27.992037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:58:27.992223: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:58:27.992326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:58:27.992463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:58:27.992569: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:58:27.992643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-8786\n",
      "I0602 11:58:27.992746 140292327446336 saver.py:1410] Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-8786\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0602 11:58:28.234511 140292327446336 session_manager.py:526] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0602 11:58:28.298032 140292327446336 session_manager.py:529] Done running local_init_op.\n",
      "INFO:tensorflow:Starting evaluation at 2023-06-02-02:58:28\n",
      "I0602 11:58:28.369858 140292327446336 evaluation.py:451] Starting evaluation at 2023-06-02-02:58:28\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "I0602 11:58:34.816113 140292327446336 evaluation.py:163] Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "I0602 11:58:37.309210 140292327446336 evaluation.py:163] Evaluation [2/3]\n",
      "2023-06-02 11:58:39.801254: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-06-02 11:58:39.820075: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "I0602 11:58:39.833710 140292327446336 evaluation.py:163] Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2023-06-02-02:58:39\n",
      "I0602 11:58:39.833965 140292327446336 evaluation.py:457] Finished evaluation at 2023-06-02-02:58:39\n",
      "INFO:tensorflow:Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "I0602 11:59:28.037994 140292327446336 evaluation.py:190] Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "INFO:tensorflow:Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-9481\n",
      "I0602 11:59:28.039024 140292327446336 evaluation.py:199] Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-9481\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0602 11:59:28.039719 140292327446336 monitored_session.py:240] Graph was finalized.\n",
      "2023-06-02 11:59:28.040097: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:59:28.040282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:59:28.040384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:59:28.040516: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:59:28.040619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 11:59:28.040695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-9481\n",
      "I0602 11:59:28.040801 140292327446336 saver.py:1410] Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-9481\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0602 11:59:28.573196 140292327446336 session_manager.py:526] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0602 11:59:28.636089 140292327446336 session_manager.py:529] Done running local_init_op.\n",
      "INFO:tensorflow:Starting evaluation at 2023-06-02-02:59:28\n",
      "I0602 11:59:28.707357 140292327446336 evaluation.py:451] Starting evaluation at 2023-06-02-02:59:28\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "I0602 11:59:35.544679 140292327446336 evaluation.py:163] Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "I0602 11:59:38.030051 140292327446336 evaluation.py:163] Evaluation [2/3]\n",
      "2023-06-02 11:59:40.387387: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "I0602 11:59:40.405921 140292327446336 evaluation.py:163] Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2023-06-02-02:59:40\n",
      "I0602 11:59:40.408487 140292327446336 evaluation.py:457] Finished evaluation at 2023-06-02-02:59:40\n",
      "2023-06-02 11:59:40.422457: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "I0602 12:00:28.085995 140292327446336 evaluation.py:190] Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "INFO:tensorflow:Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-10000\n",
      "I0602 12:00:28.087017 140292327446336 evaluation.py:199] Found new checkpoint at ckpts/groove-4-bar/train/model.ckpt-10000\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0602 12:00:28.087705 140292327446336 monitored_session.py:240] Graph was finalized.\n",
      "2023-06-02 12:00:28.088094: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 12:00:28.088277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 12:00:28.088378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 12:00:28.088516: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 12:00:28.088618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-02 12:00:28.088692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21853 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "INFO:tensorflow:Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-10000\n",
      "I0602 12:00:28.088794 140292327446336 saver.py:1410] Restoring parameters from ckpts/groove-4-bar/train/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0602 12:00:28.319820 140292327446336 session_manager.py:526] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0602 12:00:28.377539 140292327446336 session_manager.py:529] Done running local_init_op.\n",
      "INFO:tensorflow:Starting evaluation at 2023-06-02-03:00:28\n",
      "I0602 12:00:28.439720 140292327446336 evaluation.py:451] Starting evaluation at 2023-06-02-03:00:28\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "I0602 12:00:34.370240 140292327446336 evaluation.py:163] Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "I0602 12:00:36.629983 140292327446336 evaluation.py:163] Evaluation [2/3]\n",
      "2023-06-02 12:00:38.990594: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-06-02 12:00:39.000045: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "I0602 12:00:39.026218 140292327446336 evaluation.py:163] Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2023-06-02-03:00:39\n",
      "I0602 12:00:39.026368 140292327446336 evaluation.py:457] Finished evaluation at 2023-06-02-03:00:39\n",
      "INFO:tensorflow:Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "I0602 12:01:28.134179 140292327446336 evaluation.py:190] Waiting for new checkpoint at ckpts/groove-4-bar/train\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"./magenta/magenta/models/music_vae/music_vae_train.py\", line 339, in <module>\n",
      "    console_entry_point()\n",
      "  File \"./magenta/magenta/models/music_vae/music_vae_train.py\", line 335, in console_entry_point\n",
      "    tf.app.run(main)\n",
      "  File \"/home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 36, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/absl/app.py\", line 308, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/absl/app.py\", line 254, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"./magenta/magenta/models/music_vae/music_vae_train.py\", line 330, in main\n",
      "    run(configs.CONFIG_MAP)\n",
      "  File \"./magenta/magenta/models/music_vae/music_vae_train.py\", line 325, in run\n",
      "    master=FLAGS.master)\n",
      "  File \"./magenta/magenta/models/music_vae/music_vae_train.py\", line 242, in evaluate\n",
      "    master=master)\n",
      "  File \"/home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/tf_slim/training/evaluation.py\", line 440, in evaluate_repeatedly\n",
      "    timeout_fn=timeout_fn):\n",
      "  File \"/home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/tf_slim/training/evaluation.py\", line 248, in checkpoints_iterator\n",
      "    checkpoint_dir, checkpoint_path, timeout=timeout)\n",
      "  File \"/home/icl1/anaconda3/envs/test/lib/python3.7/site-packages/tf_slim/training/evaluation.py\", line 197, in wait_for_new_checkpoint\n",
      "    time.sleep(seconds_to_sleep)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# We can utilize the GrooVAE model from magenta to eval the model checkpoint\n",
    "# GrooVAE is the variant of MusicVAE which specifically designed for the drum performances in MIDI dataset\n",
    "# We will take the latest checkpoint from the training directory\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=1 python3 ./magenta/magenta/models/music_vae/music_vae_train.py\\\n",
    "    --config=\"groovae_4bar\"\\\n",
    "    --run_dir=\"ckpts/groove-4-bar\"\\\n",
    "    --mode=\"eval\"\\\n",
    "    --tfds_name=\"groove/4bar-midionly\"\\\n",
    "    --cache_dataset=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b3c3f-90e5-480c-826b-5a96e848cd5d",
   "metadata": {},
   "source": [
    "#### Inference - Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246cacb9-0769-4659-8b4d-9655b3c3ff1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f38f9a90-c997-4150-a44c-60bbcc207661",
   "metadata": {},
   "source": [
    "#### Inference - Interpolate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
